services:
  litellm:
    container_name: litellm
    image: ghcr.io/berriai/litellm:main-latest
    env_file:
      - .env
    ports:
      - 4000:4000
    volumes:
      - ./config.yaml:/app/config.yaml
    command: [ "--config", "/app/config.yaml", "--port", "4000", "--num_workers", "8" ]
    networks:
      - proxy
      - internal
    environment:
      - DATABASE_URL=postgresql://${LITELLM_POSTGRES_USER}:${LITELLM_POSTGRES_PASSWORD}@litellm_db:5432/litellm
    depends_on:
      - litellm_db
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health/liveliness || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 40s

  litellm_db:
    image: postgres:alpine
    container_name: litellm_db
    restart: always
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=${LITELLM_POSTGRES_USER}
      - POSTGRES_PASSWORD=${LITELLM_POSTGRES_PASSWORD}
      - POSTGRES_DB=${LITELLM_POSTGRES_DB}
    networks:
      - internal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d litellm -U llmproxy"]
      interval: 10s
      timeout: 5s
      retries: 5

  openwebui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - 3000:8080
    volumes:
      - owui_data:/app/backend/data
      # - ./data/docs:/data/docs
    extra_hosts:
      - "host.docker.internal:host-gateway"
    env_file:
      - .env
    restart: unless-stopped
    networks:
      - proxy
      - internal
    depends_on:
      - litellm

networks:
  proxy:
    external: true
  internal:

volumes:
  postgres_data:
    name: litellm_postgres_data
  owui_data:
